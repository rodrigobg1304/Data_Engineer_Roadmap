------

# Plan 8 semanas Data Engineer

## ğŸ§­ **1. Objetivo General**

**En 8 semanas** tendrÃ¡s un **proyecto completo** que incluye:

1. Entrenamiento y serializaciÃ³n de un modelo ML.
2. API REST con FastAPI que sirva el modelo.
3. Contenedor Docker con todo el entorno.
4. GestiÃ³n de dependencias moderna con **uv**.
5. Bonus: preparaciÃ³n para escalar hacia **Airflow / Databricks / MLOps**.

------

## ğŸ—“ï¸ **2. PLAN DE ESTUDIO Y PROYECTO â€” 8 SEMANAS**

### ğŸ”¹ **SEMANA 1 â€” Entorno y Setup Moderno**

**Objetivo:** Familiarizarte con `uv`, Docker y estructura de proyecto productivo.

**Conceptos:**

- Crear entornos con `uv` â†’ [Docs oficiales](https://docs.astral.sh/uv/pip/environments/#using-a-virtual-environment)
- Crear un `Dockerfile` bÃ¡sico y entender imÃ¡genes, capas, y volÃºmenes.
- Usar VSCode + Dev Containers o PyCharm con Docker.

**PrÃ¡ctica:**

- Monta un proyecto `ml_api_project/` con esta estructura:

  ```
  ml_api_project/
  â”œâ”€â”€ app/
  â”‚   â”œâ”€â”€ main.py
  â”‚   â”œâ”€â”€ model.py
  â”‚   â””â”€â”€ utils.py
  â”œâ”€â”€ requirements.txt
  â”œâ”€â”€ Dockerfile
  â”œâ”€â”€ README.md
  â””â”€â”€ tests/
  ```

- Crea un entorno `uv`:

  ```
  uv venv
  uv pip install fastapi uvicorn scikit-learn pandas joblib
  ```

- Crea un `Dockerfile` que construya y ejecute una app FastAPI mÃ­nima:

  ```
  FROM python:3.11-slim
  WORKDIR /app
  COPY . .
  RUN pip install uvicorn fastapi
  CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
  ```

------

### ğŸ”¹ **SEMANA 2 â€” FastAPI en Profundidad**

**Objetivo:** Construir tu primera API REST funcional.

**Conceptos:**

- Rutas (`@app.get`, `@app.post`), validaciÃ³n con Pydantic, manejo de errores.
- CORS, dependencias, y documentaciÃ³n Swagger.

**PrÃ¡ctica:**

- Implementa endpoints `/ping` y `/predict`.
- Usa `Pydantic` para definir el input del modelo (por ejemplo, datos de partido de fÃºtbol o dataset simple tipo Iris).
- Agrega tests con `pytest`.

**Recursos:**

- [FastAPI Tutorial Oficial](https://fastapi.tiangolo.com/tutorial/first-steps/)
- [ArtÃ­culo de Dorian599](https://dorian599.medium.com/fastapi-getting-started-3294efe823a0)

------

### ğŸ”¹ **SEMANA 3 â€” Modelado y ML Pipeline**

**Objetivo:** Entrenar y serializar un modelo bÃ¡sico (por ejemplo, RandomForest para predicciÃ³n de goles o clasificaciÃ³n Iris).

**Conceptos:**

- Preprocesamiento (`pandas`, `sklearn.pipeline`).
- Entrenamiento y exportaciÃ³n con `joblib` o `pickle`.
- Cargar modelo desde disco en FastAPI.

**PrÃ¡ctica:**

- Crea `train_model.py` que entrene y guarde `model.joblib`.
- En `model.py`, crea funciÃ³n `load_model()` y `predict(input_data)`.

------

### ğŸ”¹ **SEMANA 4 â€” DockerizaciÃ³n del Modelo**

**Objetivo:** Contenerizar el modelo y servirlo con FastAPI.

**Conceptos:**

- Optimizar imÃ¡genes Docker (multistage builds, `.dockerignore`).
- Variables de entorno y configuraciÃ³n (`.env`, `pydantic.BaseSettings`).

**PrÃ¡ctica:**

- Actualiza el `Dockerfile` para incluir modelo preentrenado.

- Prueba levantar el contenedor con:

  ```
  docker build -t ml-api .
  docker run -p 8000:8000 ml-api
  ```

- Valida que tu API funcione en `http://localhost:8000/docs`.

------

### ğŸ”¹ **SEMANA 5 â€” Testing, Logging y CI/CD local**

**Objetivo:** Darle nivel profesional al proyecto.

**Conceptos:**

- Logs estructurados (con `logging` o `loguru`).
- Testing automÃ¡tico con `pytest` y `requests`.
- IntegraciÃ³n con GitHub Actions (workflow bÃ¡sico de CI).

**PrÃ¡ctica:**

- Agrega tests unitarios y de integraciÃ³n (API endpoints).
- AÃ±ade un `README` con badges de CI y comandos de uso.

------

### ğŸ”¹ **SEMANA 6 â€” MLOps / Deployment**

**Objetivo:** Preparar despliegue reproducible.

**Conceptos:**

- Diferencias entre desarrollo, staging y producciÃ³n.
- Build & push a DockerHub o GitHub Container Registry.
- IntroducciÃ³n a FastAPI + uvicorn + nginx si quieres escalar.

**PrÃ¡ctica:**

- Sube tu imagen a DockerHub.
- Documenta comandos de despliegue (`docker pull`, `docker run`).
- (Opcional) Despliegue en **Render**, **Railway** o **AWS Lightsail** gratis.

------

### ğŸ”¹ **SEMANA 7 â€” Spark y Databricks**

**Objetivo:** Enfocar tu perfil hacia Data Engineer puro.

**Curso sugerido:**
ğŸ‘‰ [Databricks & PySpark de 0 a Experto (Udemy)](https://www.udemy.com/course/databricks-y-apache-spark-para-big-data-de-cero-a-experto/)

**PrÃ¡ctica:**

- Aprende a leer datasets desde S3 o Azure Blob con PySpark.
- Haz una limpieza simple y guarda resultados en Parquet.
- Integra ese pipeline como paso previo al entrenamiento del modelo.

------

### ğŸ”¹ **SEMANA 8 â€” Airflow & Pipeline Completo**

**Objetivo:** Orquestar todo el flujo como harÃ­a un Data Engineer/MLOps Engineer.

**Conceptos:**

- DAGs, Operators, Tasks.
- Programar un DAG que ejecute:
  1. Ingesta de datos (Spark).
  2. Entrenamiento (Python script).
  3. Despliegue del modelo (Docker o API update).

**Recursos:**

- [Airflow Fundamentals](https://airflow.apache.org/docs/apache-airflow/stable/tutorial/fundamentals.html)
- [GuÃ­a completa en Medium](https://medium.com/@elmahfoudradwane/apache-airflow-a-complete-guide-d2e5e5dc23b0)

------

## ğŸ¯ **3. Entrega Final / Portfolio**

Crea un **repositorio GitHub profesional** con:

- `README.md` con descripciÃ³n clara, capturas del `/docs`, y arquitectura (imagen).
- Link a DockerHub o despliegue online.
- SecciÃ³n â€œStack utilizadoâ€ y â€œLecciones aprendidasâ€.

Ejemplo de estructura:

```
# ğŸš€ ML Model API with FastAPI & Docker
Este proyecto sirve un modelo de ML (RandomForest) a travÃ©s de FastAPI, totalmente dockerizado.

## Stack
- FastAPI
- Docker
- uv
- scikit-learn
- pytest
- CI/CD con GitHub Actions

## Demo
[http://yourapp.onrender.com/docs]
```

------

## ğŸ’¼ **4. Extra: PreparaciÃ³n para Entrevistas**

- Prepara storytelling: cÃ³mo tu experiencia en Huawei + este proyecto muestra tu **capacidad de construir pipelines productivos, no solo notebooks**.
- Ten a mano mÃ©tricas o diagramas (p. ej., flujo desde ingestiÃ³n â†’ entrenamiento â†’ API).
- Explica cÃ³mo escalarÃ­as esto en la nube (AWS ECS o Azure Container Apps).



# ğŸ—“ï¸ PLAN DIARIO DE 8 SEMANAS â€” â€œPROYECTO DATA ENGINEER / MLOPS DEMOâ€

------

## **ğŸ”¹ Semana 1 â€” Setup moderno: entorno, uv y Docker**

ğŸ¯ **Objetivo:** Entender `uv`, crear un entorno limpio y contenedor base.

### DÃ­a 1

- Instala `uv` siguiendo la [guÃ­a oficial](https://docs.astral.sh/uv/pip/environments/#using-a-virtual-environment).

- Crea tu entorno:

  ```
  uv venv
  source .venv/bin/activate
  uv pip install fastapi uvicorn pandas scikit-learn joblib
  ```

- Verifica dependencias:

  ```
  uv pip list
  ```

### DÃ­a 2

- Crea estructura de proyecto:

  ```
  ml_api_project/
  â”œâ”€â”€ app/
  â”‚   â”œâ”€â”€ main.py
  â”‚   â”œâ”€â”€ model.py
  â”‚   â””â”€â”€ utils.py
  â”œâ”€â”€ requirements.txt
  â”œâ”€â”€ Dockerfile
  â””â”€â”€ README.md
  ```

- AÃ±ade `fastapi` mÃ­nima en `app/main.py`:

  ```
  from fastapi import FastAPI
  
  app = FastAPI()
  
  @app.get("/ping")
  def ping():
      return {"message": "pong"}
  ```

### DÃ­a 3

- Prueba local:

  ```
  uvicorn app.main:app --reload
  ```

- Comprueba en `http://localhost:8000/docs`.

### DÃ­a 4

- Aprende Docker: lee este artÃ­culo â†’ [A Comprehensive Guide to Docker](https://medium.com/@moraneus/a-comprehensive-guide-to-docker-286d6f3ad122).

- Crea `Dockerfile`:

  ```
  FROM python:3.11-slim
  WORKDIR /app
  COPY . .
  RUN pip install fastapi uvicorn
  CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
  ```

### DÃ­a 5

- Construye y ejecuta:

  ```
  docker build -t fastapi-demo .
  docker run -p 8000:8000 fastapi-demo
  ```

- âœ… **Entrega:** API bÃ¡sica corriendo en Docker.

------

## **ğŸ”¹ Semana 2 â€” FastAPI a fondo**

ğŸ¯ **Objetivo:** Construir endpoints `/predict` y validar input con Pydantic.

### DÃ­a 1

- Lee el [tutorial oficial de FastAPI](https://fastapi.tiangolo.com/tutorial/first-steps/).

- AÃ±ade `pydantic` y define input:

  ```
  from pydantic import BaseModel
  
  class MatchData(BaseModel):
      home_team_goals: int
      away_team_goals: int
      possession: float
  ```

### DÃ­a 2

- Crea endpoint `/predict` (mock):

  ```
  @app.post("/predict")
  def predict(data: MatchData):
      result = data.home_team_goals + data.away_team_goals
      return {"prediction": result}
  ```

### DÃ­a 3

- AÃ±ade manejo de errores (HTTPException).

- Testea con `curl` o Postman:

  ```
  curl -X POST http://localhost:8000/predict -H "Content-Type: application/json" -d '{"home_team_goals":1, "away_team_goals":2, "possession":60.5}'
  ```

### DÃ­a 4

- AÃ±ade documentaciÃ³n personalizada (`description`, `summary` en endpoints).
- Revisa `/redoc`.

### DÃ­a 5

- âœ… **Entrega:** API funcional con validaciÃ³n y mock de predicciÃ³n.

------

## **ğŸ”¹ Semana 3 â€” Entrenamiento ML y pipeline**

ğŸ¯ **Objetivo:** Entrenar un modelo simple y cargarlo en FastAPI.

### DÃ­a 1

- Usa `scikit-learn` con dataset `iris`:

  ```
  from sklearn.datasets import load_iris
  from sklearn.model_selection import train_test_split
  from sklearn.ensemble import RandomForestClassifier
  import joblib
  ```

### DÃ­a 2

- Crea `train_model.py`:

  ```
  iris = load_iris()
  X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=42)
  clf = RandomForestClassifier()
  clf.fit(X_train, y_train)
  joblib.dump(clf, "model.joblib")
  ```

### DÃ­a 3

- En `app/model.py`, aÃ±ade:

  ```
  import joblib
  model = joblib.load("model.joblib")
  
  def predict(features):
      return model.predict([features]).tolist()
  ```

### DÃ­a 4

- Modifica tu endpoint `/predict` para usar el modelo:

  ```
  @app.post("/predict")
  def predict(data: MatchData):
      pred = predict([data.home_team_goals, data.away_team_goals, data.possession])
      return {"prediction": pred}
  ```

### DÃ­a 5

- âœ… **Entrega:** modelo ML funcional, cargado en FastAPI.

------

## **ğŸ”¹ Semana 4 â€” DockerizaciÃ³n completa**

ğŸ¯ **Objetivo:** Servir el modelo en contenedor reproducible.

### DÃ­a 1â€“2

- Actualiza tu `Dockerfile`:

  ```
  FROM python:3.11-slim
  WORKDIR /app
  COPY . .
  RUN pip install -r requirements.txt
  CMD ["uvicorn", "app.main:app", "--host", "0.0.0.0", "--port", "8000"]
  ```

### DÃ­a 3

- Crea `.dockerignore`:

  ```
  __pycache__
  *.pyc
  .venv
  .git
  ```

- Build y run:

  ```
  docker build -t ml-api .
  docker run -p 8000:8000 ml-api
  ```

### DÃ­a 4â€“5

- âœ… **Entrega:** imagen Docker con modelo embebido lista para subir a DockerHub.

------

## **ğŸ”¹ Semana 5 â€” Testing, Logging y CI/CD**

ğŸ¯ **Objetivo:** AÃ±adir profesionalismo y calidad.

### DÃ­a 1

- Instala `pytest` y crea `tests/test_api.py`.

  ```
  from fastapi.testclient import TestClient
  from app.main import app
  
  client = TestClient(app)
  
  def test_ping():
      response = client.get("/ping")
      assert response.status_code == 200
  ```

### DÃ­a 2â€“3

- AÃ±ade logs con `logging`:

  ```
  import logging
  logging.basicConfig(level=logging.INFO)
  logger = logging.getLogger(__name__)
  ```

### DÃ­a 4

- Crea pipeline CI bÃ¡sico en `.github/workflows/test.yml`.

### DÃ­a 5

- âœ… **Entrega:** tests + CI funcional.

------

## **ğŸ”¹ Semana 6 â€” Deployment y MLOps**

ğŸ¯ **Objetivo:** Publicar tu API en la nube.

### DÃ­a 1â€“2

- Crea cuenta en **DockerHub**.

  ```
  docker tag ml-api <usuario>/ml-api:latest
  docker push <usuario>/ml-api:latest
  ```

### DÃ­a 3â€“4

- Despliega en **Render** o **Railway**.
  (Render: selecciona â€œDeploy Dockerâ€ y conecta tu repo).

### DÃ­a 5

- âœ… **Entrega:** link pÃºblico a tu API online.

------

## **ğŸ”¹ Semana 7 â€” Spark / Databricks**

ğŸ¯ **Objetivo:** Ampliar hacia ingenierÃ­a de datos.

### DÃ­a 1â€“2

- Inicia curso de [Databricks y PySpark](https://www.udemy.com/course/databricks-y-apache-spark-para-big-data-de-cero-a-experto/).

- Instala PySpark localmente:

  ```
  uv pip install pyspark
  ```

### DÃ­a 3â€“4

- Crea mini pipeline Spark:

  ```
  from pyspark.sql import SparkSession
  spark = SparkSession.builder.appName("demo").getOrCreate()
  df = spark.read.csv("data.csv", header=True, inferSchema=True)
  df.write.parquet("output/")
  ```

### DÃ­a 5

- âœ… **Entrega:** pipeline Spark que limpia y guarda datos en Parquet.

------

## **ğŸ”¹ Semana 8 â€” Airflow & OrquestaciÃ³n**

ğŸ¯ **Objetivo:** Crear pipeline orquestado tipo producciÃ³n.

### DÃ­a 1

- Instala Airflow (versiÃ³n local o Docker Compose).
  [Airflow Quickstart](https://airflow.apache.org/docs/apache-airflow/stable/start.html)

### DÃ­a 2â€“3

- Crea DAG con tres tareas:
  1. Ingesta (Spark script)
  2. Entrenamiento (Python)
  3. Despliegue (Docker build)

### DÃ­a 4

- Programa DAG diario con `schedule_interval='@daily'`.

### DÃ­a 5

- âœ… **Entrega:** DAG funcional y documentado.

------

## ğŸ¯ RESULTADO FINAL

Al final tendrÃ¡s:
âœ… **Repositorio GitHub profesional**
âœ… **Imagen Docker publicada**
âœ… **API pÃºblica online**
âœ… **Pipeline Spark + Airflow (opcional)**
âœ… **Historial de commits demostrable**